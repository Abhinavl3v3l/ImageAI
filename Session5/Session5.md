# Session5

Quiz Breakdown

Fully connected layers do not retain 

1. Spatial Information  -   2D to 1D looses all spatial information
2. Translational Invariance  -  a one and a 45 degree edge is same for dense layers
3. Rotational  Invariance -  a one and a 45 degree edge is same for dense layers
4. Size Invariance - 
5. Illumination Invariance.

![](https://i.stack.imgur.com/iY5n5.png)

## Batch Normalization and Regularization





- Redistributing data
- Normalization  vs Regularization
- How we normalize
- What network reacts to image with normalized vs un-normalized data.
- Normalizes data or values so the bowl would be circular and not extended in certain axis.
- Understanding CS - **Batch Normalization** solves a problem called **Internal Covariate shift** 
  - BN affects relu is good way
- Regularization 
- 